{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:48:10.252700Z","iopub.execute_input":"2023-09-28T11:48:10.253057Z","iopub.status.idle":"2023-09-28T11:48:23.614017Z","shell.execute_reply.started":"2023-09-28T11:48:10.252991Z","shell.execute_reply":"2023-09-28T11:48:23.613166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gdown\n\ntrain_url = 'https://drive.google.com/uc?id=1-4WfgSQLMIMxl-vrqDjZTMiwb1w3qiJq'\nval_url = 'https://drive.google.com/uc?id=1w9CuqPi3DbvbCN9DFGwLPeyYsXJvbjzk'\n#the problem is that the whole file train+val is too big for kaggle!\n#as kaggle gives 4.9gb of stage and the dataset is 2.8 gb\n#since we are using zipped file first.\n#https://drive.google.com/open?id=1-4WfgSQLMIMxl-vrqDjZTMiwb1w3qiJq --training\n#https://drive.google.com/open?id=1w9CuqPi3DbvbCN9DFGwLPeyYsXJvbjzk --validation\noutput_train = 'dataset_train.zip'\noutput_val = 'dataset_val.zip'\ngdown.download(train_url, output_train, quiet=False)\n#os.remove(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:48:23.616759Z","iopub.execute_input":"2023-09-28T11:48:23.617144Z","iopub.status.idle":"2023-09-28T11:48:37.967680Z","shell.execute_reply.started":"2023-09-28T11:48:23.617092Z","shell.execute_reply":"2023-09-28T11:48:37.966847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef create_dir(dir):\n  if not os.path.exists(dir):\n    os.makedirs(dir)\n    print(\"Created Directory : \", dir)\n    return\ncreate_dir(\"/kaggle/working/data_aug/train\")\n!unzip -q /kaggle/working/dataset_train.zip -d /kaggle/working/data_aug/train","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:48:37.969067Z","iopub.execute_input":"2023-09-28T11:48:37.969394Z","iopub.status.idle":"2023-09-28T11:48:59.513877Z","shell.execute_reply.started":"2023-09-28T11:48:37.969328Z","shell.execute_reply":"2023-09-28T11:48:59.513036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:48:59.515710Z","iopub.execute_input":"2023-09-28T11:48:59.516053Z","iopub.status.idle":"2023-09-28T11:48:59.526340Z","shell.execute_reply.started":"2023-09-28T11:48:59.516004Z","shell.execute_reply":"2023-09-28T11:48:59.525717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name=\"/kaggle/working/dataset_train.zip\"\nos.remove(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:48:59.527522Z","iopub.execute_input":"2023-09-28T11:48:59.527807Z","iopub.status.idle":"2023-09-28T11:49:00.717208Z","shell.execute_reply.started":"2023-09-28T11:48:59.527761Z","shell.execute_reply":"2023-09-28T11:49:00.716289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:00.718766Z","iopub.execute_input":"2023-09-28T11:49:00.719348Z","iopub.status.idle":"2023-09-28T11:49:00.730769Z","shell.execute_reply.started":"2023-09-28T11:49:00.719293Z","shell.execute_reply":"2023-09-28T11:49:00.729885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gdown.download(val_url, output_val, quiet=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:00.732068Z","iopub.execute_input":"2023-09-28T11:49:00.732381Z","iopub.status.idle":"2023-09-28T11:49:04.302671Z","shell.execute_reply.started":"2023-09-28T11:49:00.732320Z","shell.execute_reply":"2023-09-28T11:49:04.301923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:04.304136Z","iopub.execute_input":"2023-09-28T11:49:04.304535Z","iopub.status.idle":"2023-09-28T11:49:04.310833Z","shell.execute_reply.started":"2023-09-28T11:49:04.304469Z","shell.execute_reply":"2023-09-28T11:49:04.310014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_dir(\"/kaggle/working/data_aug/val\")\n!unzip -q /kaggle/working/dataset_val.zip -d /kaggle/working/data_aug/val","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:04.312112Z","iopub.execute_input":"2023-09-28T11:49:04.312413Z","iopub.status.idle":"2023-09-28T11:49:11.754109Z","shell.execute_reply.started":"2023-09-28T11:49:04.312352Z","shell.execute_reply":"2023-09-28T11:49:11.753282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/data_aug\")\nfile_name=\"/kaggle/working/dataset_val.zip\"\nos.remove(file_name)","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:11.755701Z","iopub.execute_input":"2023-09-28T11:49:11.756028Z","iopub.status.idle":"2023-09-28T11:49:11.866448Z","shell.execute_reply.started":"2023-09-28T11:49:11.755980Z","shell.execute_reply":"2023-09-28T11:49:11.864873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:11.867765Z","iopub.execute_input":"2023-09-28T11:49:11.868066Z","iopub.status.idle":"2023-09-28T11:49:13.287124Z","shell.execute_reply.started":"2023-09-28T11:49:11.868021Z","shell.execute_reply":"2023-09-28T11:49:13.286392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello everyone!! Hope everything is fine and you are enjoying things on Kaggle as usual. The rage for competing on Kaggle should never end. \nMachine Learning and Deep Learning have a huge scope in healthcare but applying them in healthcare isn't that simple. The stake is very high. It's more than just a `classification` problem. But if applied very carefully, it can benefit the world in enormous ways. **And as a Machine learning engineer, it's our responsibility to help people as much as we can in all possible ways.**\n\nPneumonia is a very common disease. It can be either: 1) Bacterial pneumonia  2) Viral Pneumonia  3) Mycoplasma pneumonia   and 4) Fungal pneumonia.\nThis dataset consists pneumonia samples belonging to the first two classes.  The dataset consists of only very few samples and that too unbalanced. The aim of this kernel is to develop a robust deep learning model from scratch on this limited amount of data. We all know that deep learning models are data hungry but if you know how things work, you can build good models even with a limited amount of data. ","metadata":{"_uuid":"a2109c7cb70175a53791577e1f5974c06340642e"}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport glob\nimport h5py\nimport shutil\nimport imgaug as aug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimg\nimport imgaug.augmenters as iaa\nfrom os import listdir, makedirs, getcwd, remove\nfrom os.path import isfile, join, abspath, exists, isdir, expanduser\nfrom PIL import Image\nfrom pathlib import Path\nfrom skimage.io import imread\nfrom skimage.transform import resize\nfrom keras.models import Sequential, Model\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport cv2\nfrom keras import backend as K\ncolor = sns.color_palette()\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-28T11:49:13.288236Z","iopub.execute_input":"2023-09-28T11:49:13.288535Z","iopub.status.idle":"2023-09-28T11:49:17.677476Z","shell.execute_reply.started":"2023-09-28T11:49:13.288489Z","shell.execute_reply":"2023-09-28T11:49:17.676654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reproducibility is a great concern when doing deep learning. There was a good discussion on `KaggleNoobs` slack regarding this. We will set a numer of things in order to make sure that the results are almost reproducible(if not fully). ","metadata":{"_uuid":"a5869d878e9a5fc94ab5eb05cbd24d67bab88378"}},{"cell_type":"code","source":"import tensorflow as tf\n\n# Set the seed for hash based operations in python\nos.environ['PYTHONHASHSEED'] = '0'\n\n# Set the numpy seed\nnp.random.seed(111)\n\n# Disable multi-threading in tensorflow ops\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n\n# Set the random seed in tensorflow at graph level\ntf.set_random_seed(111)\n\n# Define a tensorflow session with above session configs\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n\n# Set the session in keras\nK.set_session(sess)\n\n# Make the augmentation sequence deterministic\naug.seed(111)","metadata":{"_uuid":"ee4c0df90c4dfe5f2e770cb3ecf05c849fa6cd7a","_cell_guid":"9957d783-26d0-4e21-b8cb-8d1412b66bdd","execution":{"iopub.status.busy":"2023-09-28T11:49:17.678746Z","iopub.execute_input":"2023-09-28T11:49:17.679238Z","iopub.status.idle":"2023-09-28T11:49:22.297542Z","shell.execute_reply.started":"2023-09-28T11:49:17.679182Z","shell.execute_reply":"2023-09-28T11:49:22.296724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is divided into three sets: 1) train set    2) validation set    and 3) test set.  Let's grab the dataset   ","metadata":{"_uuid":"d488f59675eda4dcdab8aaafaae7263a6128c047"}},{"cell_type":"code","source":"# Define path to the data directory\n#data_dir = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\ndata_dir=\"abc\"\n# Path to train directory (Fancy pathlib...no more os.path!!)\n#train_dir = data_dir / 'train'\ntrain_dir=\"/kaggle/working/data_aug/train/content/data/content/FINAL_AUG_DATA/Train/\"\n# Path to validation directory\n#val_dir = data_dir / 'val'\nval_dir =\"/kaggle/working/data_aug/val/content/data/content/FINAL_AUG_DATA/Val/\"\n# Path to test directory\n#test_dir = data_dir / 'test'\ntest_dir = \"/kaggle/working/data_aug/val/content/data/content/FINAL_AUG_DATA/Val/\"","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-09-28T11:49:22.299078Z","iopub.execute_input":"2023-09-28T11:49:22.299393Z","iopub.status.idle":"2023-09-28T11:49:22.307619Z","shell.execute_reply.started":"2023-09-28T11:49:22.299342Z","shell.execute_reply":"2023-09-28T11:49:22.306632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will first go through the training dataset. We will do some analysis on that, look at some of the samples, check the number of samples for each class, etc.  Lets' do it.\n\nEach of the above directory contains two sub-directories:\n* `NORMAL`: These are the samples that describe the normal (no pneumonia) case.\n* `VIRAL PNEUMONIA`: This directory contains those samples that are the pneumonia cases.\n* `COVID-19`: This directory contains those samples that are the coronavirus cases.","metadata":{"_uuid":"ea5f2dd005e3c268697ec712b87ada66f8c76d5e"}},{"cell_type":"code","source":"# Get the path to the normal and pneumonia sub-directories\nfrom os.path import join\nnormal_cases_dir =join(train_dir,'NORMAL')\ncovid_cases_dir=join(train_dir,'COVID-19')\npneumonia_cases_dir = join(train_dir,'Viral Pneumonia')\n\nprint(normal_cases_dir)\n# Get the list of all the images\n\"\"\"\nnormal_cases = normal_cases_dir.glob('*')\npneumonia_cases = pneumonia_cases_dir.glob('*')\ncovid_cases = covid_cases_dir.glob('*')\n\"\"\"\nnormal_cases = glob.glob(str(normal_cases_dir)+\"/*\")\npneumonia_cases = glob.glob(str(pneumonia_cases_dir)+\"/*\")\ncovid_cases = glob.glob(str(covid_cases_dir)+\"/*\")\n\n# An empty list. We will insert the data into this list in (img_path, label) format\ntrain_data = []\n\n# Go through all the normal cases. The label for these cases will be 0\nfor img in normal_cases:\n    train_data.append((img,0))\n\n# Go through all the pneumonia cases. The label for these cases will be 1\nfor img in pneumonia_cases:\n    train_data.append((img, 1))\n    \n# Go through all the coronavirus cases. The label for these cases will be 2\nfor img in covid_cases:\n    train_data.append((img, 2))\n    \n\n# Get a pandas dataframe from the data we have in our list \ntrain_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n\n# Shuffle the data \ntrain_data = train_data.sample(frac=1.).reset_index(drop=True)\n\n# How the dataframe looks like?\ntrain_data.head()","metadata":{"_uuid":"2bad0cc6b4292d08526c433fd86fea32e31e361e","_cell_guid":"d2a81adb-fe02-4ec9-bd62-9d183598cf3b","execution":{"iopub.status.busy":"2023-09-28T11:49:22.310885Z","iopub.execute_input":"2023-09-28T11:49:22.311217Z","iopub.status.idle":"2023-09-28T11:49:22.442521Z","shell.execute_reply.started":"2023-09-28T11:49:22.311162Z","shell.execute_reply":"2023-09-28T11:49:22.441761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:22.443710Z","iopub.execute_input":"2023-09-28T11:49:22.444184Z","iopub.status.idle":"2023-09-28T11:49:22.449988Z","shell.execute_reply.started":"2023-09-28T11:49:22.444129Z","shell.execute_reply":"2023-09-28T11:49:22.449163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"9000*0.8","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:49:22.451313Z","iopub.execute_input":"2023-09-28T11:49:22.451746Z","iopub.status.idle":"2023-09-28T11:49:22.464377Z","shell.execute_reply.started":"2023-09-28T11:49:22.451694Z","shell.execute_reply":"2023-09-28T11:49:22.463724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How many samples for each class are there in the dataset?","metadata":{"_uuid":"c827c5bbbefcea77a93ae7fb2141534e297dce08"}},{"cell_type":"code","source":"# Get the counts for each class\ncases_count = train_data['label'].value_counts()\nprint(cases_count)\n\n# Plot the results \nplt.figure(figsize=(10,8))\nsns.barplot(x=cases_count.index, y= cases_count.values)\nplt.title('Number of cases', fontsize=14)\nplt.xlabel('Case type', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)','Covid(2)'])\nplt.show()","metadata":{"_uuid":"7339a4be8a266b0d8b13f1f6136370b711691f7a","_cell_guid":"55a1b867-5499-4460-a435-8145c1bdff4e","execution":{"iopub.status.busy":"2023-09-28T11:49:22.465658Z","iopub.execute_input":"2023-09-28T11:49:22.465991Z","iopub.status.idle":"2023-09-28T11:49:22.652911Z","shell.execute_reply.started":"2023-09-28T11:49:22.465926Z","shell.execute_reply":"2023-09-28T11:49:22.650395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see the data is highly imbalanced. We have almost with thrice pneumonia cases here as compared to the normal cases. This situation is very normal when it comes to medical data. The data will always be imbalanced. either there will be too many normal cases or there will be too many cases with the disease. \n\nLet's look at how a normal case is different from that of a pneumonia case. We will look at somes samples from our training data itself.","metadata":{"_uuid":"4b873e7b24eca6e9840901656e0182743c2a639d"}},{"cell_type":"code","source":"# Get few samples for both the classes\npneumonia_samples = (train_data[train_data['label']==1]['image'].iloc[:5]).tolist()\nnormal_samples = (train_data[train_data['label']==0]['image'].iloc[:5]).tolist()\ncovid_samples = (train_data[train_data['label']==2]['image'].iloc[:5]).tolist()\n# Concat the data in a single list and del the above two list\nsamples = pneumonia_samples + normal_samples+covid_samples\ndel pneumonia_samples, normal_samples,covid_samples\n\n# Plot the data \nf, ax = plt.subplots(3,5, figsize=(30,15))\nfor i in range(15):\n    img = imread(samples[i])\n    ax[i//5, i%5].imshow(img, cmap='gray')\n    if i<5:\n        ax[i//5, i%5].set_title(\"Pneumonia\")\n    elif i<10:\n        ax[i//5, i%5].set_title(\"Normal\")\n    else:\n        ax[i//5, i%5].set_title(\"COVID-19\")\n    \n    ax[i//5, i%5].axis('off')\n    ax[i//5, i%5].set_aspect('auto')\nplt.show()","metadata":{"_uuid":"977e06568660798f12288e7c29bba0fb35646c31","_cell_guid":"294e9c52-2d9a-427c-ba19-c8f3a28b0b65","execution":{"iopub.status.busy":"2023-09-28T11:49:22.655977Z","iopub.execute_input":"2023-09-28T11:49:22.656226Z","iopub.status.idle":"2023-09-28T11:49:25.194087Z","shell.execute_reply.started":"2023-09-28T11:49:22.656180Z","shell.execute_reply":"2023-09-28T11:49:25.193240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look carefully, then there are some cases where you won't be able to differentiate between a normal case and a pneumonia case with the naked eye. There is one case in the above plot, at least for me ,which is too much confusing. If we can build a robust classifier, it would be a great assist to the doctor too.","metadata":{"_uuid":"bded42af5b1e4da6bbb12157c5f7935c0be87b5b"}},{"cell_type":"markdown","source":"### Preparing validation data\nWe will be defining a generator for the training dataset later in the notebook but as the validation data is small, so I can read the images and can load the data without the need of a generator.  This is exactly what the code block given below is doing.","metadata":{"_uuid":"422892036677e8ab0cd5eb1df2a55752611de2a6"}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the path to the sub-directories\nnormal_cases_dir = join(val_dir,'NORMAL')\npneumonia_cases_dir =join(val_dir,'Viral Pneumonia')\ncovid_cases_dir=join(val_dir,'COVID-19')\n\n# Get the list of all the images\n#normal_cases = normal_cases_dir.glob('*.jpeg')\n#pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n\n\n\nnormal_cases = glob.glob(str(normal_cases_dir)+\"/*\")\npneumonia_cases = glob.glob(str(pneumonia_cases_dir)+\"/*\")\ncovid_cases = glob.glob(str(covid_cases_dir)+\"/*\")\n\n\n\n\n# List that are going to contain validation images data and the corresponding labels\nvalid_data = []\nvalid_labels = []\n\n\n# Some images are in grayscale while majority of them contains 3 channels. So, if the image is grayscale, we will convert into a image with 3 channels.\n# We will normalize the pixel values and resizing all the images to 224x224 \n\n# Normal cases\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    try:\n        img = cv2.resize(img, (224,224),interpolation = cv2.INTER_AREA)\n    except:\n        raise ERR\n        continue\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(0, num_classes=3)\n    valid_data.append(img)\n    valid_labels.append(label)\n                      \n# Pneumonia cases        \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    try:\n        img = cv2.resize(img, (224,224),interpolation = cv2.INTER_AREA)\n    except:\n        raise ERR\n        continue\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(1, num_classes=3)\n    valid_data.append(img)\n    valid_labels.append(label)\n    \n    \n# Coronavirus cases        \nfor img in covid_cases:\n    img = cv2.imread(str(img))\n    try:\n        img = cv2.resize(img, (224,224),interpolation = cv2.INTER_AREA)\n    except:\n        raise ERR\n        continue\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(2, num_classes=3)\n    valid_data.append(img)\n    valid_labels.append(label)\n# Convert the list into numpy arrays\nvalid_data = np.array(valid_data)\nvalid_labels = np.array(valid_labels)\n\nprint(\"Total number of validation examples: \", valid_data.shape)\nprint(\"Total number of labels:\", valid_labels.shape)","metadata":{"_uuid":"fa1af0731ad6ff427ff72d9d029a2a414f4d11a3","_cell_guid":"4ef15f9a-b9bc-4e00-9ff5-928e287c526e","execution":{"iopub.status.busy":"2023-09-28T11:49:25.195297Z","iopub.execute_input":"2023-09-28T11:49:25.195683Z","iopub.status.idle":"2023-09-28T11:50:06.003683Z","shell.execute_reply.started":"2023-09-28T11:49:25.195638Z","shell.execute_reply":"2023-09-28T11:50:06.002837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"9000*0.2","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:50:06.004997Z","iopub.execute_input":"2023-09-28T11:50:06.005475Z","iopub.status.idle":"2023-09-28T11:50:06.011735Z","shell.execute_reply.started":"2023-09-28T11:50:06.005421Z","shell.execute_reply":"2023-09-28T11:50:06.010981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentation\nData augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But augmentation can be much more helpful where the dataset is imbalanced. You can generate different samples of undersampled class in order to try to balance the overall distribution. \n\nI like [imgaug](https://imgaug.readthedocs.io/en/latest/) a lot. It comes with a very clean api and you can do hell of augmentations with it. It's worth exploring!!\nIn the next code block, I will define a augmentation sequence. You will notice `Oneof` and it does exactly that. At each iteration, it will take one augmentation technique out of the three and will apply that on the samples ","metadata":{"_uuid":"eba8d9dec36028e4a49ed3b9ffc84f0d94ab861e"}},{"cell_type":"code","source":"# Augmentation sequence \nseq = iaa.OneOf([\n    #iaa.Fliplr(), # horizontal flips\n    #iaa.Affine(rotate=20), # roatation\n    iaa.Multiply((1.2, 1.5))]) #random brightness","metadata":{"_uuid":"628f0edaefe4da1d55e2e5f022314ebfeed1a2e6","_cell_guid":"b38439c7-208b-4495-b994-f9c89449005b","execution":{"iopub.status.busy":"2023-09-28T11:50:06.013063Z","iopub.execute_input":"2023-09-28T11:50:06.013575Z","iopub.status.idle":"2023-09-28T11:50:06.029589Z","shell.execute_reply.started":"2023-09-28T11:50:06.013524Z","shell.execute_reply":"2023-09-28T11:50:06.028798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training data generator \nHere I will define a very simple data generator. You can do more than this if you want but I think at this point, this is more than enough I need.","metadata":{"_uuid":"2cbc776086dc1afc6b5d79346f2c29e3451e7273"}},{"cell_type":"code","source":"def data_gen(data, batch_size):\n    # Get total number of samples in the data\n    n = len(data)\n    steps = n//batch_size\n    \n    # Define two numpy arrays for containing batch data and labels\n    batch_data = np.zeros((batch_size, 224, 224, 3), dtype=np.float32)\n    batch_labels = np.zeros((batch_size,3), dtype=np.float32)\n\n    # Get a numpy array of all the indices of the input data\n    indices = np.arange(n)\n    \n    # Initialize a counter\n    i =0\n    while True:\n        np.random.shuffle(indices)\n        # Get the next batch \n        count = 0\n        next_batch = indices[(i*batch_size):(i+1)*batch_size]\n        for j, idx in enumerate(next_batch):\n            img_name = data.iloc[idx]['image']\n            label = data.iloc[idx]['label']\n            \n            # one hot encoding\n            encoded_label = to_categorical(label, num_classes=3)\n            # read the image and resize\n            img = cv2.imread(str(img_name))\n            img = cv2.resize(img, (224,224))\n            \n            # check if it's grayscale\n            #if img.shape[2]==1:\n            #    img = np.dstack([img, img, img])\n            \n            # cv2 reads in BGR mode by default\n            orig_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            # normalize the image pixels\n            orig_img = img.astype(np.float32)/255.\n            \n            batch_data[count] = orig_img\n            batch_labels[count] = encoded_label\n            \n            # generating more samples of the undersampled class\n            if label==0 and count < batch_size-2:\n            #    aug_img1 = seq.augment_image(img)\n            #    aug_img2 = seq.augment_image(img)\n            #    aug_img1 = cv2.cvtColor(aug_img1, cv2.COLOR_BGR2RGB)\n            #    aug_img2 = cv2.cvtColor(aug_img2, cv2.COLOR_BGR2RGB)\n            #    aug_img1 = aug_img1.astype(np.float32)/255.\n            #    aug_img2 = aug_img2.astype(np.float32)/255.\n\n            #    batch_data[count+1] = aug_img1\n            #    batch_labels[count+1] = encoded_label\n            #    batch_data[count+2] = aug_img2\n            #    batch_labels[count+2] = encoded_label\n                count +=2\n            \n            else:\n                count+=1\n            \n            if count==batch_size-1:\n                break\n            \n        i+=1\n        yield batch_data, batch_labels\n            \n        if i>=steps:\n            i=0","metadata":{"_uuid":"2becec172e3fde7839e671416932c36254aafb68","_cell_guid":"12f786dc-67b6-409c-95e2-6ccd6ffef39f","execution":{"iopub.status.busy":"2023-09-28T11:50:06.031004Z","iopub.execute_input":"2023-09-28T11:50:06.031332Z","iopub.status.idle":"2023-09-28T11:50:06.184593Z","shell.execute_reply.started":"2023-09-28T11:50:06.031268Z","shell.execute_reply":"2023-09-28T11:50:06.183688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model \n\nThis is the best part. If you look at other kernels on this dataset, everyone is busy doing transfer learning and fine-tuning. **You should transfer learn but wisely**.  We will be doing partial transfer learning and rest of the model will be trained from scratch. I will explain this in detail but before that, I would love to share one of the best practices when it comes to building deep learning models from scratch on limited data.\n\n1. Choose a simple architecture.\n2. Initialize the first few layers from a network that is pretrained on imagenet. This is because first few layers capture general details like color blobs, patches, edges, etc. Instead of randomly initialized weights for these layers, it would be much better if you fine tune them.\n3. Choose layers that introduce a lesser number of parameters. For example, `Depthwise SeparableConv` is a good replacement for `Conv` layer. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel, it captures more information.  `Xception` a powerful network, is built on top of such layers only. You can read about `Xception` and `Depthwise Separable Convolutions` in [this](https://arxiv.org/abs/1610.02357) paper.\n4. Use batch norm with convolutions.  As the network becomes deeper, batch norm start to play an important role.\n5. Add dense layers with reasonable amount of neurons. Train with a higher learning rate and experiment with the number of neurons in the dense layers. Do it for the depth of your network too. \n6. Once you know a good depth, start training your network with a lower learning rate along with decay.  \n\nThis is all that I have done in the next code block.\n","metadata":{"_uuid":"92ed415eafc68ddc33d639e12d80d665d5e47340"}},{"cell_type":"code","source":"def build_model2():\n    input_img = Input(shape=(224,224,3), name='ImageInput')\n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv0_1')(input_img)\n    x = Conv2D(32, (3,3), activation='relu', padding='same', name='Conv0_2')(x)\n    x = MaxPooling2D((2,2), name='pool0')(x)\n    \n    x = SeparableConv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(x)\n    x = SeparableConv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling2D((2,2), name='pool1')(x)\n    \n    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling2D((2,2), name='pool2')(x)\n    \n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling2D((2,2), name='pool3')(x)\n    \n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n    x = BatchNormalization(name='bn3')(x)\n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n    x = BatchNormalization(name='bn4')(x)\n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n    x = MaxPooling2D((2,2), name='pool4')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(1024, activation='relu', name='fc1')(x)\n    x = Dropout(0.7, name='dropout1')(x)\n    x = Dense(512, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(3, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","metadata":{"_uuid":"6288e9af327d72237ec04364ae5d14ff1d490061","_cell_guid":"118d98a9-c5f4-4288-bf41-a67080657af2","execution":{"iopub.status.busy":"2023-09-28T11:50:06.186169Z","iopub.execute_input":"2023-09-28T11:50:06.186490Z","iopub.status.idle":"2023-09-28T11:50:06.330061Z","shell.execute_reply.started":"2023-09-28T11:50:06.186426Z","shell.execute_reply":"2023-09-28T11:50:06.329188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    input_img = Input(shape=(224,224,3), name='ImageInput')\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv2D(64, (3,3), activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling2D((2,2), name='pool1')(x)\n    \n    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv2D(128, (3,3), activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling2D((2,2), name='pool2')(x)\n    \n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    x = SeparableConv2D(256, (3,3), activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling2D((2,2), name='pool3')(x)\n    \n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_1')(x)\n    x = BatchNormalization(name='bn3')(x)\n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_2')(x)\n    x = BatchNormalization(name='bn4')(x)\n    x = SeparableConv2D(512, (3,3), activation='relu', padding='same', name='Conv4_3')(x)\n    x = MaxPooling2D((2,2), name='pool4')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(1024, activation='relu', name='fc1')(x)\n    x = Dropout(0.7, name='dropout1')(x)\n    x = Dense(512, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(3, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-09-28T11:50:06.331552Z","iopub.execute_input":"2023-09-28T11:50:06.331937Z","iopub.status.idle":"2023-09-28T11:50:06.448414Z","shell.execute_reply.started":"2023-09-28T11:50:06.331850Z","shell.execute_reply":"2023-09-28T11:50:06.447683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model =  build_model()\nmodel.summary()","metadata":{"_uuid":"b34d477061729a390c947d4b9ee2473b068294ec","_cell_guid":"37863ed3-1814-437d-8db1-7d56283f0a87","execution":{"iopub.status.busy":"2023-09-28T11:50:06.450139Z","iopub.execute_input":"2023-09-28T11:50:06.450498Z","iopub.status.idle":"2023-09-28T11:50:07.207463Z","shell.execute_reply.started":"2023-09-28T11:50:06.450430Z","shell.execute_reply":"2023-09-28T11:50:07.206664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will initialize the weights of first two convolutions with imagenet weights,","metadata":{"_uuid":"e5df7b732a3c7d4ad5d2e2b3da3f4c6a9e1ab01f"}},{"cell_type":"code","source":"# Open the VGG16 weight file\nf = h5py.File('../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', 'r')\n\n# Select the layers for which you want to set weight.\n\nw,b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1']['block1_conv1_b_1:0']\nmodel.layers[1].set_weights = [w,b]\n\nw,b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2']['block1_conv2_b_1:0']\nmodel.layers[2].set_weights = [w,b]\n\nw,b = f['block2_conv1']['block2_conv1_W_1:0'], f['block2_conv1']['block2_conv1_b_1:0']\nmodel.layers[4].set_weights = [w,b]\n\nw,b = f['block2_conv2']['block2_conv2_W_1:0'], f['block2_conv2']['block2_conv2_b_1:0']\nmodel.layers[5].set_weights = [w,b]\n\nf.close()\nmodel.summary()    ","metadata":{"_uuid":"6ca55c4ce3478d075c36dbf29d3d1890ef6d301f","_cell_guid":"556f2cd2-a824-498a-afad-824dc4d7a951","execution":{"iopub.status.busy":"2023-09-28T11:50:07.208703Z","iopub.execute_input":"2023-09-28T11:50:07.209002Z","iopub.status.idle":"2023-09-28T11:50:07.272392Z","shell.execute_reply.started":"2023-09-28T11:50:07.208952Z","shell.execute_reply":"2023-09-28T11:50:07.271650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# opt = RMSprop(lr=0.0001, decay=1e-6)\n#change the optimizer Adam to SGD with lr = .1\n#labels converted to categorical?\nopt = Adam(lr=0.0001, decay=1e-5)\n#from keras_adabound import AdaBound\n#loss=tf.keras.losses.CategoricalCrossentropy()\n\n#model.compile(optimizer=AdaBound(lr=1e-3, final_lr=0.1), loss=model_loss)\n#opt = Adam(lr=0.1, decay=1e-2)\nes = EarlyStopping(patience=5)\nchkpt = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)\n#model.compile(loss='binary_crossentropy', metrics=['accuracy'],optimizer=opt)\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt)\n#metrics=['accuracy', 'categorical_accuracy', 'precision', 'recall']","metadata":{"_uuid":"15295c609c87c11f943aa0ed24055ca65bbd57f2","_cell_guid":"87cd9cae-b836-4470-84fd-56a83f5e82c8","execution":{"iopub.status.busy":"2023-09-28T12:04:20.078958Z","iopub.execute_input":"2023-09-28T12:04:20.079318Z","iopub.status.idle":"2023-09-28T12:04:20.121139Z","shell.execute_reply.started":"2023-09-28T12:04:20.079257Z","shell.execute_reply":"2023-09-28T12:04:20.120471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16\nnb_epochs = 3\n\n# Get a train data generator\ntrain_data_gen = data_gen(data=train_data, batch_size=batch_size)\n\n# Define the number of training steps\nnb_train_steps = train_data.shape[0]//batch_size\n\nprint(\"Number of training and validation steps: {} and {}\".format(nb_train_steps, len(valid_data)))","metadata":{"_uuid":"890ed0a4de401a42bf54bbecb58f105a360d3c7d","_cell_guid":"0c4b5c2d-d344-465c-8d20-3c628ed19292","execution":{"iopub.status.busy":"2023-09-28T12:04:23.248836Z","iopub.execute_input":"2023-09-28T12:04:23.249163Z","iopub.status.idle":"2023-09-28T12:04:23.257600Z","shell.execute_reply.started":"2023-09-28T12:04:23.249103Z","shell.execute_reply":"2023-09-28T12:04:23.256573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have commented out the training step as of now as it will train the network again while rendering the notebook and I would have to wait for an hour or so which I don't want to. I have uploaded the weights of the best model I achieved so far. Feel free to use it for further fine-tuning of the network. We will load those weights and will run the inference on the test set using those weights only. But...but for your reference, I will attach the screenshot of the training steps here.","metadata":{"_uuid":"931d5e031670cfce0ee6bebfc37db0787dbc66ee"}},{"cell_type":"code","source":"# Fit the model\nhistory = model.fit_generator(train_data_gen, epochs=nb_epochs, steps_per_epoch=nb_train_steps,\n                              validation_data=(valid_data, valid_labels),callbacks=[es, chkpt],\n                              class_weight='balanced')","metadata":{"_uuid":"0cfc4e08fdea211ff50c400b143dad1be126e226","_cell_guid":"7005e3d2-0b62-4f14-8def-ef0af971b6fd","execution":{"iopub.status.busy":"2023-09-28T12:05:58.447289Z","iopub.execute_input":"2023-09-28T12:05:58.447639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"mymodel.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'best_model_todate')","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:12:01.749951Z","iopub.execute_input":"2023-09-28T12:12:01.750267Z","iopub.status.idle":"2023-09-28T12:12:01.754050Z","shell.execute_reply.started":"2023-09-28T12:12:01.750208Z","shell.execute_reply":"2023-09-28T12:12:01.753270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  os.path.getsize(\"best_model_todate\")/(1024*1024) #megabytes","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:12:02.968576Z","iopub.execute_input":"2023-09-28T12:12:02.968876Z","iopub.status.idle":"2023-09-28T12:12:02.972692Z","shell.execute_reply.started":"2023-09-28T12:12:02.968819Z","shell.execute_reply":"2023-09-28T12:12:02.971942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:12:06.175736Z","iopub.execute_input":"2023-09-28T12:12:06.176086Z","iopub.status.idle":"2023-09-28T12:12:06.181979Z","shell.execute_reply.started":"2023-09-28T12:12:06.176026Z","shell.execute_reply":"2023-09-28T12:12:06.181105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model weights\n# model.load_weights(\"../input/xray-best-model/best_model/best_model.hdf5\")\n# model.load_weights(\"/kaggle/input/xray-best-model/best_model.hdf5\")\nmodel.load_weights(\"/kaggle/working/mymodel.h5\")","metadata":{"_uuid":"f8af9aee46133b89e02c825cea1a0fcdf801eac0","execution":{"iopub.status.busy":"2023-09-28T12:12:28.880411Z","iopub.execute_input":"2023-09-28T12:12:28.880735Z","iopub.status.idle":"2023-09-28T12:12:29.504433Z","shell.execute_reply.started":"2023-09-28T12:12:28.880676Z","shell.execute_reply":"2023-09-28T12:12:29.503511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:12:32.485633Z","iopub.execute_input":"2023-09-28T12:12:32.485970Z","iopub.status.idle":"2023-09-28T12:12:32.490884Z","shell.execute_reply.started":"2023-09-28T12:12:32.485885Z","shell.execute_reply":"2023-09-28T12:12:32.490052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing test data\nnormal_cases_dir = join(test_dir,'NORMAL')\npneumonia_cases_dir = join(test_dir,'Viral Pneumonia')\ncovid_cases_dir=join(test_dir,'COVID-19')\n\nnormal_cases = glob.glob(str(normal_cases_dir) + \"/*\")\npneumonia_cases = glob.glob(str(pneumonia_cases_dir) + \"/*\")\ncovid_cases = glob.glob(str(covid_cases_dir)+\"/*\")\n\ntest_data = []\ntest_labels = []\n\nfor img in normal_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (224,224))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    else:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(0, num_classes=3)\n    test_data.append(img)\n    test_labels.append(label)\n                      \nfor img in pneumonia_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (224,224))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    else:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(1, num_classes=3)\n    test_data.append(img)\n    test_labels.append(label)\n    \nfor img in covid_cases:\n    img = cv2.imread(str(img))\n    img = cv2.resize(img, (224,224))\n    if img.shape[2] ==1:\n        img = np.dstack([img, img, img])\n    else:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32)/255.\n    label = to_categorical(2, num_classes=3)\n    test_data.append(img)\n    test_labels.append(label)\n\ntest_data = np.array(test_data)\ntest_labels = np.array(test_labels)\n\nprint(\"Total number of test examples: \", test_data.shape)\nprint(\"Total number of labels:\", test_labels.shape)\n\n","metadata":{"_uuid":"135d485c94679ae8942fc0e372091d914b752b55","_cell_guid":"7e366caa-87e1-4761-aaca-5d87dee2605f","execution":{"iopub.status.busy":"2023-09-28T12:12:44.414186Z","iopub.execute_input":"2023-09-28T12:12:44.414493Z","iopub.status.idle":"2023-09-28T12:13:13.934777Z","shell.execute_reply.started":"2023-09-28T12:12:44.414436Z","shell.execute_reply":"2023-09-28T12:13:13.933562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation on test dataset\ntest_loss, test_score = model.evaluate(test_data, test_labels, batch_size=16)\nprint(\"Loss on test set: \", test_loss)\nprint(\"Accuracy on test set: \", test_score)","metadata":{"_uuid":"8032be06a247d736a041f83f0f78d74ee0310a86","_cell_guid":"65afc715-9f5b-4a3a-b2a9-053bd271ba94","execution":{"iopub.status.busy":"2023-09-28T12:13:32.728110Z","iopub.execute_input":"2023-09-28T12:13:32.728422Z","iopub.status.idle":"2023-09-28T12:13:37.742393Z","shell.execute_reply.started":"2023-09-28T12:13:32.728366Z","shell.execute_reply":"2023-09-28T12:13:37.741039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\npreds = model.predict(test_data, batch_size=16)\npreds = np.argmax(preds, axis=-1)\n\n# Original labels\norig_test_labels = np.argmax(test_labels, axis=-1)\n\nprint(orig_test_labels.shape)\nprint(preds.shape)","metadata":{"_uuid":"5bfc1e8956055abdb5d5bb3399509864aa635ba6","execution":{"iopub.status.busy":"2023-09-28T12:13:45.041325Z","iopub.execute_input":"2023-09-28T12:13:45.041643Z","iopub.status.idle":"2023-09-28T12:13:50.142544Z","shell.execute_reply.started":"2023-09-28T12:13:45.041576Z","shell.execute_reply":"2023-09-28T12:13:50.141684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When a particular problem includes an imbalanced dataset, then accuracy isn't a good  metric to look for. For example, if your dataset contains 95 negatives and 5 positives, having a model with 95% accuracy doesn't make sense at all. The classifier might label every example as negative and still achieve 95% accuracy. Hence,  we need to look for alternative metrics. **Precision** and **Recall** are really good metrics for such kind of problems. \n\nWe will get the confusion matrix from our predictions and see what is the recall and precision of our model.","metadata":{"_uuid":"5384c318ee60802ce2ec73aea2632b75586c34bb","_cell_guid":"761b7db9-0529-4fb5-a049-114d164cdd67","trusted":true}},{"cell_type":"code","source":"# Get the confusion matrix\ncm  = confusion_matrix(orig_test_labels, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, alpha=0.8,cmap=plt.cm.Blues)\nplt.xticks(range(3), ['Normal', 'Pneumonia', 'Corona'], fontsize=16)\nplt.yticks(range(3), ['Normal', 'Pneumonia', 'Corona'], fontsize=16)\nplt.show()","metadata":{"_uuid":"7a0944856f2d0666cf7c4fd90ad3e690ed17b2e7","execution":{"iopub.status.busy":"2023-09-28T12:13:56.074624Z","iopub.execute_input":"2023-09-28T12:13:56.074990Z","iopub.status.idle":"2023-09-28T12:13:56.281117Z","shell.execute_reply.started":"2023-09-28T12:13:56.074919Z","shell.execute_reply":"2023-09-28T12:13:56.280125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:13:59.894032Z","iopub.execute_input":"2023-09-28T12:13:59.894335Z","iopub.status.idle":"2023-09-28T12:13:59.899515Z","shell.execute_reply.started":"2023-09-28T12:13:59.894279Z","shell.execute_reply":"2023-09-28T12:13:59.898662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confusion_matrix_for(cls, cm):\n    TP = cm[cls, cls]\n    FN = cm[cls].sum() - TP\n    FP = cm[:, cls].sum() - TP\n    TN = cm.sum() - TP - FN - FP\n    return np.array([[TP, FN], [FP, TN]])","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:16:04.968178Z","iopub.execute_input":"2023-09-28T12:16:04.968507Z","iopub.status.idle":"2023-09-28T12:16:04.976230Z","shell.execute_reply.started":"2023-09-28T12:16:04.968447Z","shell.execute_reply":"2023-09-28T12:16:04.975451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_for(0, cm).ravel()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:16:30.314669Z","iopub.execute_input":"2023-09-28T12:16:30.315036Z","iopub.status.idle":"2023-09-28T12:16:30.321024Z","shell.execute_reply.started":"2023-09-28T12:16:30.314956Z","shell.execute_reply":"2023-09-28T12:16:30.320052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_for(1, cm).ravel()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:17:04.601168Z","iopub.execute_input":"2023-09-28T12:17:04.601774Z","iopub.status.idle":"2023-09-28T12:17:04.609360Z","shell.execute_reply.started":"2023-09-28T12:17:04.601717Z","shell.execute_reply":"2023-09-28T12:17:04.608470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_for(2, cm).ravel()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:16:49.598556Z","iopub.execute_input":"2023-09-28T12:16:49.598903Z","iopub.status.idle":"2023-09-28T12:16:49.607376Z","shell.execute_reply.started":"2023-09-28T12:16:49.598826Z","shell.execute_reply":"2023-09-28T12:16:49.606559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cls in range(cm.shape[0]):\n    print(f'[Class {cls} vs others]')\n    TP, FN, FP, TN = confusion_matrix_for(cls, cm).ravel()\n    print(f'TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN}')\n    # compute your metrics (your code in the question)\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:17:12.422504Z","iopub.execute_input":"2023-09-28T12:17:12.422845Z","iopub.status.idle":"2023-09-28T12:17:12.432880Z","shell.execute_reply.started":"2023-09-28T12:17:12.422779Z","shell.execute_reply":"2023-09-28T12:17:12.431997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Precision and Recall for class 0\nTN, FP, FN, TP = confusion_matrix_for(0, cm).ravel()\nprecision_0 = TP/(TP+FP)\nrecall_0 = TP/(TP+FN)\n\nprint(\"Recall of the model is {:.2f}\".format(recall_0))\nprint(\"Precision of the model is {:.2f}\".format(precision_0))","metadata":{"_uuid":"7429c904fed42eacc83dc385a7c83b1c47b64183","execution":{"iopub.status.busy":"2023-09-28T12:23:47.380161Z","iopub.execute_input":"2023-09-28T12:23:47.380514Z","iopub.status.idle":"2023-09-28T12:23:47.389285Z","shell.execute_reply.started":"2023-09-28T12:23:47.380454Z","shell.execute_reply":"2023-09-28T12:23:47.388433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Precision and Recall for class 1\nTN, FP, FN, TP = confusion_matrix_for(1, cm).ravel()\nprecision_1 = TP/(TP+FP)\nrecall_1 = TP/(TP+FN)\n\nprint(\"Recall of the model is {:.2f}\".format(recall_1))\nprint(\"Precision of the model is {:.2f}\".format(precision_1))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:24:25.622881Z","iopub.execute_input":"2023-09-28T12:24:25.623244Z","iopub.status.idle":"2023-09-28T12:24:25.632146Z","shell.execute_reply.started":"2023-09-28T12:24:25.623183Z","shell.execute_reply":"2023-09-28T12:24:25.631339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Precision and Recall for class 2\nTN, FP, FN, TP = confusion_matrix_for(2, cm).ravel()\nprecision_2 = TP/(TP+FP)\nrecall_2 = TP/(TP+FN)\n\nprint(\"Recall of the model is {:.2f}\".format(recall_2))\nprint(\"Precision of the model is {:.2f}\".format(precision_2))","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:24:57.624431Z","iopub.execute_input":"2023-09-28T12:24:57.624755Z","iopub.status.idle":"2023-09-28T12:24:57.633527Z","shell.execute_reply.started":"2023-09-28T12:24:57.624694Z","shell.execute_reply":"2023-09-28T12:24:57.632692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import model_from_json\nfrom keras.models import load_model\n\n# serialize model to JSON\n#  the keras model which is trained is defined as 'model' in this example\nmodel_json = model.to_json()\n\n\nwith open(\"model_2.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\n# serialize weights to HDF5\nmodel.save_weights(\"model_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:26:29.106002Z","iopub.execute_input":"2023-09-28T12:26:29.106359Z","iopub.status.idle":"2023-09-28T12:26:29.594251Z","shell.execute_reply.started":"2023-09-28T12:26:29.106301Z","shell.execute_reply":"2023-09-28T12:26:29.593298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"/kaggle/working\")","metadata":{"execution":{"iopub.status.busy":"2023-09-28T12:26:51.709685Z","iopub.execute_input":"2023-09-28T12:26:51.710017Z","iopub.status.idle":"2023-09-28T12:26:51.715776Z","shell.execute_reply.started":"2023-09-28T12:26:51.709955Z","shell.execute_reply":"2023-09-28T12:26:51.715049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice!!! So, our model has a 98% recall. In such problems, a good recall value is expected. But if you notice, the precision is only 80%. This is one thing to notice. Precision and Recall follows a trade-off, and you need to find a point where your recall, as well as your precision, is more than good but both can't increase simultaneously. \n\nThat's it folks!! I hope you enjoyed this kernel. Happy Kaggling!!","metadata":{"_uuid":"12900d098502134c4a51527cdf5a2c1203ecaa36","trusted":true}}]}